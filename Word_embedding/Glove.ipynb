{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "659189df-73ca-47f8-a70a-568a752dd0d0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1. 사전 훈련된 벡터를 가진 txt 파일 준비\n",
    "https://drive.google.com/file/d/1yHGtccC2FV3_d6C6_Q4cozYSOgA7bG-e/view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242a83a0-8fcc-4a73-8e3f-a71f917bf06a",
   "metadata": {},
   "source": [
    "## 2. txt 파일에 있는 단어와 벡터들을 glove dictionary 에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bec9f7c-930b-457b-bb5c-f3512115c067",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = dict()\n",
    "f = open('./glove.txt')\n",
    "for line in f:    #enter 기준 하나하나 접근\n",
    "    values = line.split()   # 한 줄 내에서 띄어쓰기 단위로 쪼개서 list형태로 만든다음 values에 저장\n",
    "    word = values[0]       # 그 중에 맨 앞에 있는 값 (word 를 뜻하는 정보가 들어있나 봄)\n",
    "    vector = asarray(values[1:], dtype='float32')   # asarray : 참조 전달  // values에 전달된 list \n",
    "    glove[word] = vector # glove라는 사전 저장소에 word(현재 처리하고 있는 단어)라는 key 에 vector(해당하는 사전 훈련된 단어 벡터)를 value로써 전달  => (word,vector) 형태로 저장됨\n",
    "f.close()   # 다 glove에 넣으면 닫기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c7bad0-f312-4ed9-827f-978c36496d7a",
   "metadata": {},
   "source": [
    "코드 흐름 제대로 이해하기 위한 세부 코드 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0286e96b-ce87-4cec-a7a3-f46d252cad22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.TextIOWrapper name='test.txt' mode='r' encoding='utf8'>\n",
      "<class 'str'>\n",
      "['테스트', '입니다.']\n",
      "<class 'list'>\n",
      "테스트 입니다.\n",
      "\n",
      "<class 'str'>\n",
      "['테스트', '2', '입니다.']\n",
      "<class 'list'>\n",
      "테스트 2 입니다.\n"
     ]
    }
   ],
   "source": [
    "t= open('test.txt','r',encoding=\"utf8\")\n",
    "print(t)\n",
    "for line in t:\n",
    "    print(type(line))\n",
    "    v = line.split()\n",
    "    print(v)\n",
    "    print(type(v))\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dbd685-0305-48cf-865d-b752d1b8ac1d",
   "metadata": {},
   "source": [
    "## 결과\n",
    "\n",
    "이전에 배웠을 때에는\n",
    "lines = t.readlines() 를 통해 \n",
    "txt 내의 문장을 띄어쓰기 를 기준으로 분리하여, list 형태로 저장한 다음, \n",
    "이 lines를 for 문을 이용해\n",
    "for line in lines:\n",
    "    \"line을 다루는 코드\"  => 주의! 각 list내의 원소의 오른쪽 끝에는 \\n을 포함하고 있으므로 print를 하거나 할 때, end=\"\"를 해주어야 한다.\n",
    "    \n",
    "형식으로 접근하였지만,\n",
    "지금 코드를 보니\n",
    "그냥 open 한 것이, for문의 저장소 역할을 해 낼 수 있음을 볼 수 있다.  ==> 옛날에 range(숫자) 만으로도 for 문의 저장소 역할을 했던 것과 유사하다!\n",
    "\n",
    "이 for 문을 통해 txt 내의 문장을 각각 접근 가능함을 확인하였고, print에서 end=\"\"를 해줄 필요가 없는 것을 보아 문장 끝에 \\n을 포함하고 있지 않음을 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0742446c-f115-40b8-9543-dd14f9baeb02",
   "metadata": {},
   "source": [
    "## 3. vocabulary에 있는 토큰들의 벡터를 가져와 embedding matrix에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202ff114-f934-4dc2-a15a-cc58b11802ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 300)) #300차원의 임베딩 매트릭스 생성\n",
    "\n",
    "for index, word in enumerate(vocabulary): #vocabulary에 있는 토큰들을 하나씩 넘겨줍니다.\n",
    "    if word in glove: #넘겨 받은 토큰이 glove에 존재하면(이미 훈련이 된 토큰이라는 뜻)\n",
    "        embedding_vector = glove[word] #해당 토큰에 해당하는 vector를 불러오고\n",
    "        embedding_mxtrix[i] = embedding_vector #해당 위치의 embedding_mxtrix에 저장합니다.\n",
    "    else:\n",
    "        print(\"glove 없는 단어입니다.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360fca5b-b3b2-4a42-9c2b-a1a3a4b6e0e9",
   "metadata": {},
   "source": [
    "## 4. keras embedding layer에 embedding_matrix를 가중치로 주어 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a00030d-2984-49fa-8830-b2f540369639",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 300,weights = [embedding_matrx], input_length = max_len))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
