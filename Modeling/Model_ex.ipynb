{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd1d557f-57a3-4467-a853-5b5bdc04c828",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. 간단한 전처리 + 형태소 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d5ec25-ed8b-4a00-b86d-fde89b8d022a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text_list):\n",
    "    \n",
    "    stopwords = ['을', '를', '이', '가', '은', '는', 'null']\n",
    "    tokenizer = Okt()\n",
    "    \n",
    "    for text in tqdm.tqdm(text_list):  # tqdm으로 진도 표시  import tqdm 해주어야 함\n",
    "        txt = re.sub('[^가-힣a-z]', ' ', text.lower())    # 모두 소문자화 시킨 후, 소문자 영어와 한국어 빼고 모두 공백처리\n",
    "        token = tokenizer.morphs(txt) # 형태소 변환하여 token에 저장\n",
    "        token = [t for t in token if t not in stopwords or type(t) != float]   # stopword나 실수(숫자)이면 token으로 넣지 않는다.\n",
    "        \n",
    "    return token, tokenizer\n",
    "\n",
    "train['token'], okt = text_preprocessing(train['content'])   # train data의 content를 받아와 token과 okt 에 정보 전달"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd2fbb9-f2df-471b-bce1-23effedf67ce",
   "metadata": {},
   "source": [
    "## 2. vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b10dd15-f37c-4465-b5a1-5e1131a21b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2sequence(train_text, max_len=1000):\n",
    "    \n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(train_text)   # train_Text : 형태소 단위로 공백을 두고 문자열type으로 된 여러 문장들이 list 안에 하나하나씩 들어 있음   \n",
    "                                        #=>  여기에 사용된 모든 문장의 형태소들(공백을 두고 띄어져잇는 조각들)에 해당하는 token들에게 정수 이름 부여\n",
    "    train_X_seq = tokenizer.texts_to_sequences(train_text) # 부여된 정수로 표시된 문장\n",
    "    vocab_size = len(tokenizer.word_index) + 1 # .word_index : 모든 형태소들을 사전 형태로 담은 변수 => 0부터 시작하므로 서로다른 모든 vocab(형태소) 개수는 +1 해주어야\n",
    "    print('vocab_size : ', vocab_size)\n",
    "    X_train = pad_sequences(train_X_seq, maxlen = max_len) # 모든 길이를 1000으로 맞추고, 짧은 문장들은 뒤에 남는 공간을 0으로 처리한다 (padding)\n",
    "    return X_train, vocab_size, tokenizer\n",
    "\n",
    "train_y = train['info']\n",
    "train_X, vocab_size, vectorizer = text2sequence(train['token'], max_len = 100)\n",
    "print(train_X.shape, train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179bc0cd-e619-4ff6-b72a-19715f817996",
   "metadata": {},
   "source": [
    "## 3. Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3973c79-3efd-45f6-b4fe-2efe5150af1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary = True)\n",
    "embedding_matrix = np.zeros((vocab_size, 300))  # 300 차원의 embedding 생성\n",
    "\n",
    "for index, word in enumerate(vocabulary):\n",
    "    if word in word2vec\n",
    "        embedding_vector = word2vec[word] \n",
    "        embedding_mxtrix[i] = embedding_vector \n",
    "    else:\n",
    "        print(\"word2vec에 없는 단어입니다.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba75b95-f02f-4ce5-b509-ffdc6d8001a0",
   "metadata": {},
   "source": [
    "## 4. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7877c89c-9ae1-4a80-9e63-cd321100ccd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM(vocab_size, max_len=1000):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 300,weights = [embedding_matrx], input_length = max_len)) #임베딩 가중치 적용 코드\n",
    "    model.add(SpatialDropout1D(0.3))  # dropout 설정(0.3)\n",
    "    model.add(LSTM(64))  # 64개 노드\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu', kernel_regularizer = regularizers.l2(0.001)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics='accuracy')\n",
    "    model.summary()\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
